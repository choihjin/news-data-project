# Dockerfile-airflow
FROM apache/airflow:2.10.5-python3.11

USER root
# airflow 사용자를 docker 그룹에 추가
RUN groupadd -r docker && usermod -aG docker airflow

# 시스템 패키지 설치
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        openjdk-17-jdk \
        curl \
        gnupg \
        lsb-release \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Java 설치 + 폰트 설치 같이
RUN apt update && apt install -y \
    openjdk-17-jdk \
    curl \
    wget \
    unzip \
    fontconfig \
    fonts-nanum \
    libfreetype6-dev \
    libpng-dev \
    libjpeg-dev \
    python3-dev \
    gcc \
    g++ \
    make \
    pkg-config

# Spark 설치
ENV SPARK_VERSION=3.5.4
ENV SPARK_HOME=/opt/spark
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="${JAVA_HOME}/bin:${SPARK_HOME}/bin:${PATH}"

RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz && \
    tar -xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop3 $SPARK_HOME && \
    rm spark-${SPARK_VERSION}-bin-hadoop3.tgz

# 폰트 캐시 갱신
RUN fc-cache -fv

USER airflow

# Python 패키지 설치
COPY docker/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Flink Kafka Connector 설치
COPY batch/scripts/config/flink-sql-connector-kafka-3.3.0-1.20.jar /opt/airflow/scripts/config/