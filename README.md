# ğŸ“° ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì„ë² ë”© ê¸°ë°˜ ë¶„ì„ í”Œë«í¼

## ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ:**
ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , í…ìŠ¤íŠ¸ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ ë‰´ìŠ¤ ì¶”ì²œ ë° ì¸ì‚¬ì´íŠ¸ ë¶„ì„ì´ ê°€ëŠ¥í•œ í”Œë«í¼ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

**í•µì‹¬ í‚¤ì›Œë“œ:**
`RSS`, `í¬ë¡¤ë§`, `pgvector`, `ë‰´ìŠ¤ ì„ë² ë”©`, `PostgreSQL`, `Kafka`, `Flink`, `Spark`, `Airflow`, `Elasticsearch`

## ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜
![alt text](img/pipeline.png)

## âš™ï¸ ì „ì²´ êµ¬ì„± ìš”ì†Œ ë° ì—­í• 

### âœ… Kafka Producer

* RSS í”¼ë“œë¥¼ ìˆ˜ì§‘í•˜ê³  ê¸°ì‚¬ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•˜ì—¬ Kafka Topic(`news`)ì— JSON ì „ì†¡
* íŒŒì¼: `producer_hankyung.py`, `producer_aitimes.py`

### âœ… Flink Consumer

* Kafka ë©”ì‹œì§€ë¥¼ PyFlinkë¡œ ìˆ˜ì‹ í•˜ì—¬ OpenAI ì „ì²˜ë¦¬ í›„ PostgreSQLì— ì €ì¥
* ì£¼ìš” ì²˜ë¦¬ í•­ëª©: í‚¤ì›Œë“œ ì¶”ì¶œ, ì„ë² ë”© ìƒì„±, ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
* íŒŒì¼: `consumer.py`

### âœ… OpenAI ì „ì²˜ë¦¬ ëª¨ë“ˆ

| í•¨ìˆ˜ëª…                                 | ì„¤ëª…              |
| ----------------------------------- | --------------- |
| `transform_extract_keywords(text)`  | í‚¤ì›Œë“œ 5ê°œ ì¶”ì¶œ       |
| `transform_to_embedding(text)`      | ë³¸ë¬¸ ì„ë² ë”© (1536ì°¨ì›) |
| `transform_classify_category(text)` | IT ë¶„ì•¼ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜   |

### âœ… PostgreSQL

* í…Œì´ë¸”ëª…: `news_article`
* ì„ë² ë”© ë²¡í„° ì €ì¥ì„ ìœ„í•œ `pgvector` í™•ì¥ ì‚¬ìš©

```sql
CREATE TABLE news_article (
  id SERIAL PRIMARY KEY,
  title VARCHAR,
  writer VARCHAR,
  write_date TIMESTAMP,
  category VARCHAR,
  content TEXT,
  url VARCHAR UNIQUE,
  keywords JSONB,
  embedding VECTOR(1536),
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### âœ… Elasticsearch

* ê²€ìƒ‰ ì—”ì§„ ì—­í• 
* `migration.py` ë˜ëŠ” Airflow DAG í†µí•´ ë™ê¸°í™”

```json
"mappings": {
  "properties": {
    "title":     { "type": "text" },
    "content":   { "type": "text" },
    "writer":    { "type": "text" },
    "url":       { "type": "keyword" },
    "category":  { "type": "keyword" },
    "keywords":  { "type": "keyword" },
    "write_date":{ "type": "date" }
  }
}
```
```bash
# migration.py ì‹¤í–‰ (ì´ˆê¸° 1íšŒ)
# PostgreSQL ì£¼ì†Œ localhost â†’ 192.168.210.75 ìˆ˜ì • í•„ìš”
$ python migration.py
```

### âœ… Spark (Batch ë¶„ì„)

* `spark_daily_report.py`: ì¼ë³„ PDF ë¦¬í¬íŠ¸ ìƒì„± (í‚¤ì›Œë“œ, ì¹´í…Œê³ ë¦¬, ì‹œê°„ëŒ€ ë¶„ì„)

### âœ… Airflow

* DAG êµ¬ì„±:

  * `streaming_dag.py`: ì‹¤ì‹œê°„ íë¦„ ê´€ë¦¬ìš© DAG (ì˜ˆë¹„)
  * `daily_report_dag.py`: Spark ê¸°ë°˜ ë¦¬í¬íŠ¸ ìë™í™” (ë§¤ì¼ 9ì‹œ ì‹¤í–‰)
  * `sync_postgres_to_es.py`: 10ë¶„ ë‹¨ìœ„ Elasticsearch ë™ê¸°í™” (upsert ê¸°ë°˜)
* ì‹¤í–‰ ìœ„ì¹˜: `/opt/airflow/dags/`

```bash
# Airflow ì´ˆê¸°í™” ë° ì‹¤í–‰
$ sudo docker compose up airflow-init
$ sudo docker compose up -d

# UI ì ‘ì†: http://localhost:8080 (ID/PW: airflow/airflow)
```

### âœ… Kibana
![alt text](img/image.png)

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì˜ˆì‹œ (ìƒìœ„: data-pjt)

```bash
batch/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ daily_report_dag.py
â”‚   â”œâ”€â”€ streaming_dag.py
â”‚   â”œâ”€â”€ sync_postgres_to_es.py
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ spark_daily_report.py
â”œâ”€â”€ data/
â”œâ”€â”€ logs/
â”œâ”€â”€ output/
```

## ğŸ“§ ì´ë©”ì¼ ì „ì†¡ ì„¤ì • ì˜ˆì‹œ (.env)

```env
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_USER=heeju531341@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=ì•±ë¹„ë°€ë²ˆí˜¸
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_MAIL_FROM=heeju531341@gmail.com
```

## ğŸš€ í–¥í›„ í™•ì¥ ê³„íš

* pgvector ê¸°ë°˜ ìœ ì‚¬ ë‰´ìŠ¤ ì¶”ì²œ API (`/similar-news`)
* Full-Text ê²€ìƒ‰ ë° í•„í„°ë§ ê¸°ëŠ¥
* Spark ê¸°ë°˜ ì‹œê°í™” ë¦¬í¬íŠ¸ ê³ ë„í™”
* RSS ìˆ˜ì§‘ ëŒ€ìƒ ì–¸ë¡ ì‚¬ ë° ì¹´í…Œê³ ë¦¬ í™•ì¥
* Kafka + Flink ì¥ì•  ëŒ€ì‘ ë° ìë™ ì¬ì‹œì‘ ë¡œì§ êµ¬ì„±
