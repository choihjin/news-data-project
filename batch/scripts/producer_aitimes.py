from kafka import KafkaProducer
import json
import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time

# Kafka ì„¤ì •
KAFKA_BROKER = 'kafka:9092'
TOPIC = 'news'

producer = KafkaProducer(
    bootstrap_servers=KAFKA_BROKER,
    value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')
)

RSS_FEED_URL = "https://www.aitimes.com/rss/allArticle.xml"

def get_article_content(url: str) -> str:
    try:
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        res = requests.get(url, headers=headers, timeout=5)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        content_div = soup.select_one('#article-view-content-div')
        
        if not content_div:
            print(f"â— ë³¸ë¬¸ div ì—†ìŒ: {url}")
            return None
            
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for tag in content_div.select('div.article_ad, .copyright, script, style'):
            tag.decompose()
            
        # ê° ë‹¨ë½ì„ ì¶”ì¶œí•˜ì—¬ ì •ì œ
        paragraphs = []
        for p in content_div.find_all(['p', 'div']):
            text = p.get_text(strip=True)
            if text and not any(skip in text.lower() for skip in ['â–¶', 'Â©', 'ì €ì‘ê¶Œì', 'ë¬´ë‹¨ì „ì¬', 'ë°°í¬ê¸ˆì§€', 'â–¼', 'ê´€ë ¨ê¸°ì‚¬', 'ê¸°ì']):
                # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±° ë° ë¬¸ì¥ ì •ë¦¬
                text = ' '.join(text.split())
                paragraphs.append(text)
        
        # ë¬¸ë‹¨ ì‚¬ì´ì— ë¹ˆ ì¤„ì„ ì¶”ê°€í•˜ì—¬ ê²°í•©
        content_text = '\n\n'.join(paragraphs)
        
        # ë³¸ë¬¸ ê¸¸ì´ ì²´í¬
        if len(content_text.strip()) < 100:
            print(f"â›” ë³¸ë¬¸ ë„ˆë¬´ ì§§ìŒ: {url}")
            return None
            
        return content_text.strip()
    except Exception as e:
        print(f"â— í¬ë¡¤ë§ ì—ëŸ¬: {url} â†’ {e}")
        return None

def send_to_kafka(article):
    try:
        producer.send(TOPIC, value=article)
        print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] âœ… Kafka ì „ì†¡ ì™„ë£Œ: {article['title']}")
    except Exception as e:
        print(f"âŒ Kafka ì „ì†¡ ì‹¤íŒ¨: {e}")

seen_links = set()

def main():
    print("ğŸ“¡ AIíƒ€ì„ìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘\n")
    feed = feedparser.parse(RSS_FEED_URL)
    print(f"ì´ ê¸°ì‚¬ ìˆ˜: {len(feed.entries)}")

    for entry in feed.entries:
        if entry.link in seen_links:
            continue
        seen_links.add(entry.link)

        try:
            published = entry.get("published", "2025-01-01 00:00:00")
            published_dt = datetime.strptime(published, "%Y-%m-%d %H:%M:%S")
        except:
            published_dt = datetime.now()

        content = get_article_content(entry.link)
        if content is None:
            print(f"â›” ë³¸ë¬¸ ëˆ„ë½ â†’ Kafka ì „ì†¡ ìƒëµ: {entry.link}")
            continue

        article = {
            "title": entry.title,
            "writer": entry.get("author", "AIíƒ€ì„ìŠ¤"),
            "write_date": published_dt.strftime("%Y-%m-%d %H:%M:%S"),
            "content": content,
            "url": entry.link,
            "keywords": []
        }

        send_to_kafka(article)
        time.sleep(0.5)  # ìš”ì²­ ê°„ê²©ì„ 0.5ì´ˆë¡œ ì¤„ì„
        
    producer.flush()
    print("\nğŸ› ï¸ Kafka Producer ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    main()